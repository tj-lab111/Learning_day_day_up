<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>强化学习的数学原理 - 深度全解 (完整版)</title>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.9.0/mermaid.min.js"></script>
    <script>mermaid.initialize({ startOnLoad: true, theme: 'base', securityLevel: 'loose', flowchart: { useMaxWidth: true, htmlLabels: true } });</script>

    <script>
        window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' }, options: { renderActions: { addMenu: [0, '', ''] } } };
    </script>
    <script id="MathJax-script" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>

    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #34495e;
            --accent-color: #007bff;
            --bg-color: #f8f9fa;
            --text-color: #212529;
            --sidebar-width: 300px;
            --sidebar-bg: #fff;
            --heading-font: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        }

        body {
            font-family: "Georgia", "Times New Roman", Times, serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            margin: 0;
            line-height: 1.8;
            transition: margin-left 0.3s ease;
        }

        nav {
            width: var(--sidebar-width);
            background-color: var(--sidebar-bg);
            border-right: 1px solid #dee2e6;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            top: 0;
            left: 0;
            z-index: 1000;
            transition: transform 0.3s ease;
            box-shadow: 2px 0 5px rgba(0, 0, 0, 0.05);
        }

        body.sidebar-closed nav {
            transform: translateX(-100%);
        }

        body.sidebar-closed main {
            margin-left: 0;
            max-width: 1200px;
            padding: 3rem 10%;
        }

        #sidebar-toggle {
            position: fixed;
            top: 15px;
            left: 15px;
            z-index: 1001;
            background: rgba(255, 255, 255, 0.95);
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 6px 10px;
            cursor: pointer;
            font-size: 1.2rem;
            color: #333;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        #sidebar-toggle:hover {
            background: #f8f8f8;
            color: var(--accent-color);
        }

        nav .nav-header {
            padding: 1.5rem;
            padding-top: 5rem;
            background: var(--primary-color);
            color: white;
            text-align: center;
        }

        nav .nav-header h2 {
            margin: 0;
            font-size: 1.3rem;
            font-family: var(--heading-font);
            color: #fff !important;
            font-weight: 600;
        }

        nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        nav li a {
            display: block;
            padding: 12px 20px;
            color: var(--text-color);
            text-decoration: none;
            font-family: var(--heading-font);
            font-size: 0.95rem;
            border-bottom: 1px solid #f1f1f1;
            transition: background 0.2s;
        }

        nav li a:hover {
            background-color: #e9ecef;
            color: var(--accent-color);
            padding-left: 25px;
        }

        main {
            margin-left: var(--sidebar-width);
            padding: 3rem 5rem;
            max-width: 1000px;
            background-color: white;
            min-height: 100vh;
            transition: margin-left 0.3s ease, max-width 0.3s ease, padding 0.3s ease;
        }

        h1,
        h2,
        h3,
        h4 {
            font-family: var(--heading-font);
            color: var(--primary-color);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 1.8rem;
            margin-top: 3rem;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 2rem;
            color: var(--secondary-color);
        }

        p {
            margin-bottom: 1.2rem;
            text-align: justify;
        }

        .box {
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
            font-family: var(--heading-font);
            position: relative;
        }

        .box-def {
            background-color: #e3f2fd;
            border-left: 5px solid #2196f3;
        }

        .box-def strong {
            color: #1976d2;
            display: block;
            margin-bottom: 0.5rem;
        }

        .box-thm {
            background-color: #fff8e1;
            border-left: 5px solid #ffc107;
        }

        .box-thm strong {
            color: #f57f17;
            display: block;
            margin-bottom: 0.5rem;
        }

        .box-easy {
            background-color: #f1f8e9;
            border-left: 5px solid #8bc34a;
            margin-top: -1rem;
            margin-bottom: 2rem;
            border-radius: 0 0 4px 4px;
            padding-top: 2rem;
        }

        .box-easy strong {
            color: #558b2f;
            display: block;
            margin-bottom: 0.5rem;
            font-size: 0.95em;
        }

        .box-easy::before {
            content: "💡 简单说 / Plain English";
            position: absolute;
            top: -10px;
            right: 10px;
            background: #8bc34a;
            color: white;
            padding: 2px 8px;
            border-radius: 10px;
            font-size: 0.8rem;
            font-weight: bold;
        }

        .box-logic {
            background-color: #fdf2f8;
            border-left: 5px solid #db2777;
            margin: 2rem 0;
            padding: 1.5rem;
            padding-top: 2.5rem;
            border-radius: 4px;
            font-family: var(--heading-font);
            position: relative;
        }

        .box-logic::before {
            content: "🔗 章节关联 / Logic Chain";
            position: absolute;
            top: -10px;
            left: 10px;
            background: #db2777;
            color: white;
            padding: 2px 8px;
            border-radius: 10px;
            font-size: 0.8rem;
            font-weight: bold;
        }

        .box-flow {
            background-color: #e8eaf6;
            border-left: 5px solid #5c6bc0;
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            padding-top: 2rem;
            border-radius: 4px;
            font-family: var(--heading-font);
            position: relative;
            font-size: 0.95rem;
        }

        .box-flow::before {
            content: "➡️ 小节逻辑 / Section Flow";
            position: absolute;
            top: -10px;
            left: 10px;
            background: #5c6bc0;
            color: white;
            padding: 2px 8px;
            border-radius: 10px;
            font-size: 0.75rem;
            font-weight: bold;
        }

        .formula-breakdown {
            margin-top: 10px;
            padding: 10px;
            background: #fff;
            border: 1px dashed #999;
            border-radius: 4px;
            font-size: 0.9rem;
            font-family: var(--heading-font);
        }

        .formula-breakdown-title {
            display: block;
            font-weight: bold;
            color: #555;
            margin-bottom: 5px;
            border-bottom: 1px dashed #eee;
            padding-bottom: 2px;
        }

        .formula-breakdown ul {
            margin: 0;
            padding-left: 1.2rem;
            list-style-type: circle;
        }

        .formula-breakdown li {
            margin-bottom: 4px;
            color: #444;
        }

        .mermaid {
            text-align: center;
            margin: 2rem 0;
        }

        .source-tag {
            display: inline-block;
            background: #6c757d;
            color: white;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: var(--heading-font);
            font-size: 0.8rem;
            margin-bottom: 10px;
        }

        .source-link {
            color: #6c757d;
            text-decoration: none;
            font-size: 0.85rem;
            margin-left: 10px;
            font-family: var(--heading-font);
        }

        .source-link:hover {
            text-decoration: underline;
            color: var(--accent-color);
        }

        .img-caption {
            text-align: center;
            font-size: 0.9rem;
            color: #666;
            margin-top: 8px;
            font-family: var(--heading-font);
        }

        footer {
            margin-top: 5rem;
            padding-top: 2rem;
            border-top: 1px solid #eee;
            text-align: center;
            color: #6c757d;
            font-size: 0.9rem;
        }

        #loading-check {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            background: #ffeeba;
            color: #856404;
            text-align: center;
            padding: 10px;
            z-index: 9999;
            display: none;
        }
    </style>
</head>

<body>

    <div id="loading-check">⚠️ 正在加载组件...</div>
    <script>
        document.getElementById('loading-check').style.display = 'block';
        window.onload = function () { setTimeout(function () { if (window.MathJax && window.MathJax.typesetPromise) { document.getElementById('loading-check').style.display = 'none'; } else { document.getElementById('loading-check').innerText = "❌ 公式加载失败。"; } }, 4000); };
        function toggleSidebar() { document.body.classList.toggle('sidebar-closed'); }
    </script>

    <button id="sidebar-toggle" onclick="toggleSidebar()" title="收起/展开目录">☰</button>

    <nav>
        <div class="nav-header">
            <h2>课程目录</h2>
        </div>
        <ul>
            <li><a href="#intro">前言：逻辑导图</a></li>
            <li><a href="#ch1">第1章：基本概念 (MDP)</a></li>
            <li><a href="#ch2">第2章：贝尔曼方程</a></li>
            <li><a href="#ch3">第3章：贝尔曼最优方程</a></li>
            <li><a href="#ch4">第4章：动态规划 (DP)</a></li>
            <li><a href="#ch5">第5章：蒙特卡洛方法</a></li>
            <li><a href="#ch6">第6章：随机逼近与 SGD</a></li>
            <li><a href="#ch7">第7章：时序差分 (TD)</a></li>
            <li><a href="#ch8">第8章：值函数近似 (DQN)</a></li>
            <li><a href="#ch9">第9章：策略梯度 (PG)</a></li>
            <li><a href="#ch10">第10章：Actor-Critic</a></li>
        </ul>
    </nav>

    <main>
        <header>
            <h1>强化学习的数学原理</h1>
            <p style="font-size: 1.2rem; color: #555;">Mathematical Foundations of Reinforcement Learning</p>
            <p><b>讲师：</b>赵世钰 (Westlake University)</p>
            <p>
                <a href="https://www.bilibili.com/video/BV1sd4y167NS" target="_blank" class="source-link">📺 Bilibili
                    课程首页</a>
                <a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"
                    target="_blank" class="source-link">📁 此处下载 PDF 教材</a>
            </p>
            <hr>
        </header>

        <section id="intro">
            <h2>前言：学习地图</h2>
            <p>本笔记致力于将"高大上"的数学公式拆解为"通俗易懂"的语言。</p>
            <div class="mermaid">
                graph TD
                subgraph Tools ["基础工具 / Fundamental Tools"]
                Ch1["Ch1: 基本概念 (MDP)"] -->|"定义目标: G"| Ch2["Ch2: 贝尔曼方程 (V, Q)"]
                Ch2 -->|"追求最优: max"| Ch3["Ch3: 贝尔曼最优方程 (BOE)"]
                end

                subgraph Algos ["算法 / Algorithms"]
                Ch3 -->|"求解 BOE (已知模型)"| Ch4["Ch4: 动态规划 (DP)"]
                Ch4 -- "去模型化 (Model-free)" --> Ch5["Ch5: 蒙特卡洛 (MC)"]
                Ch5 -- "增量式更新 (快)" --> Ch7["Ch7: 时序差分 (TD)"]
                Ch6["Ch6: 随机逼近理论"] -->|"提供数学保障"| Ch7
                end

                subgraph Rep ["表示 / Representation"]
                Ch7 -- "解决维度灾难 (表格->函数)" --> Ch8["Ch8: 值函数近似 (DQN)"]
                end

                subgraph Policy ["策略 / Policy"]
                Ch8 -- "解决连续动作 (Value->Policy)" --> Ch9["Ch9: 策略梯度 (PG)"]
                Ch9 -- "降低方差 (引入Critic)" --> Ch10["Ch10: Actor-Critic (A2C)"]
                end

                style Ch1 fill:#f9f,stroke:#333,stroke-width:2px
                style Ch4 fill:#bbf,stroke:#333,stroke-width:2px
                style Ch8 fill:#bfb,stroke:#333,stroke-width:2px
            </div>
        </section>

        <!-- CHAPTER 1 -->
        <section id="ch1">
            <div class="source-tag">来源：第1课 Basic Concepts</div>
            <h2>第1章：基本概念 (MDP)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> 在解决问题之前，首先要用数学语言精确地<b>描述问题</b>。MDP 就是强化学习的"语言"。</p>
            </div>

            <h3>1.1 马尔可夫决策过程 (MDP)</h3>
            <div class="box-def">
                <strong>定义 1.1 (MDP 元组)</strong>
                <p>一个 MDP 由五元组 $(S, A, P, R, \gamma)$ 定义。</p>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>我们把所有游戏、生活中的决策都抽象成这五个元素：</p>
                <ul>
                    <li><b>S (状态 - 处境)</b>：你在哪？环境长啥样？比如王者荣耀里的"三路兵线位置"、"队友是否存活"。</li>
                    <li><b>A (动作 - 抉择)</b>：你能干啥？比如"进攻"、"撤退"、"打龙"。</li>
                    <li><b>P (转移 - 物理法则)</b>：你做了动作后，世界会怎么变？你按了"开火"，子弹一定能打中吗？不一定，因为有后坐力（概率性）。这就是 P 所描述的不确定性。</li>
                    <li><b>R (奖励 - 反馈)</b>：做得好有糖吃（+1分），做得差挨板子（-1分）。这是指导我们学习的唯一信号。</li>
                    <li><b>$\gamma$ (折扣 - 耐心)</b>：现在的100块肯定比明天的100块值钱。$\gamma=0$ 表示我只在乎现在爽不爽（短视）；$\gamma=1$
                        表示我极度耐心，为了未来的大饼可以饿死现在。通常我们取 0.9，表示稍微看重长远。</li>
                </ul>
            </div>

            <div class="box-flow">
                <p><b>1.1 MDP</b> 定义了游戏规则 -> 但我们需要一个"总分"来衡量好坏 -> 这就是 <b>1.2 回报 G</b>。</p>
            </div>

            <h3>1.2 回报 (Return)</h3>
            <div class="box-def">
                <strong>定义 1.2 (折扣回报)</strong>
                $$ G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$G_t$: <b>总回报</b>。不仅仅是这一步赚了多少，而是从现在开始到游戏结束，我总共能赚多少（带折扣）。RL 的终极目标就是让这个 $G_t$ 最大。</li>
                        <li>$R_{t+1}$: <b>即时奖励</b>。就像这一顿饭好不好吃。</li>
                        <li>$\gamma^k$: 越往后的奖励，打折越狠。意味着未来的大饼越不可信，越不值钱。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>想象你在玩《超级马里奥》：</p>
                <p><b>即时奖励 ($R$)</b>：你头顶了一个砖块，获得10分。开心吗？开心。但如果你为了顶这个砖块掉坑里死了，值吗？不值。</p>
                <p><b>回报 ($G$)</b>：是你从现在开始，一直到通关（或死掉）为止，吃到的所有金币总和。强化学习不care你这一步顶不顶砖块，它care的是你能不能<b>活着拿到最多金币</b>。</p>
                <p>所以，<b>R 是战术，G 是战略。</b></p>
            </div>

            <div class="box-flow">
                <p><b>1.2 回报 G</b> 是我们的目标 -> 但怎么操作才能拿到高回报？ -> 需要一个行动指南 -> 这就是 <b>1.3 策略 Policy</b>。</p>
            </div>

            <h3>1.3 策略 (Policy)</h3>
            <div class="box-def">
                <strong>定义 1.3 (策略)</strong>
                <p>策略 $\pi$ 是从状态到动作的映射函数。告诉我们在某个状态下该干什么。</p>
                <ul>
                    <li><b>确定性策略 (Deterministic):</b> $a = \pi(s)$。看到路口必定左转。</li>
                    <li><b>随机性策略 (Stochastic):</b> $\pi(a|s) = P(A_t=a | S_t=s)$。看到路口，70%概率左转，30%概率右转。</li>
                </ul>
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 核心概念：</span>
                    <ul>
                        <li><b>Trajectory (轨迹):</b> 有了策略 $\pi$ 和环境 $P$，玩一局游戏就会产生一条序列：$S_0, A_0, R_1, S_1, A_1, R_2,
                            \dots$。这就是轨迹。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p><b>策略</b>就是<b>"锦囊妙计"</b>。</p>
                <p>诸葛亮给赵云三个锦囊，告诉他"见人就拆"（确定性）或者"抓阄决定"（随机性）。这就是策略。</p>
                <p>强化学习的目的，就是从零开始，把这个空白的锦囊（$\pi$），慢慢填满，变成百战百胜的最优兵法（$\pi^*$）。</p>
            </div>
        </section>

        <!-- CHAPTER 2 -->
        <section id="ch2">
            <div class="source-tag">来源：第2课 Bellman Equation</div>
            <h2>第2章：贝尔曼方程 (Bellman Equation)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> 第1章有了"锦囊"（策略）。但这个锦囊好不好？能不能帮我拿高分（回报G）？需科学评估 -> <b>价值函数 V</b>。</p>
            </div>

            <h3>2.1 状态价值函数 $v_\pi$</h3>
            <div class="box-def">
                <strong>定义 2.1 (状态价值)</strong>
                $$ v_\pi(s) = \mathbb{E}_\pi [G_t | S_t=s] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$v_\pi(s)$: 状态 $s$ 的"身价"。在这个位置，我平均能拿多少分。</li>
                        <li>$\mathbb{E}_\pi$: <b>数学期望</b>。因为有随机性（P不确定，策略$\pi$也不确定），我们没法算准"一次"，只能算"平均水平"。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p><b>状态价值</b>就是给当前的局势"估价"。</p>
                <p>比如玩斗地主。你手里拿着"王炸+四个2"（状态 $S$）。虽然还没打完（游戏没结束，$G$ 未知），但你心里知道："这把稳了！"。你会给这个状态打个高分（比如 v=100）。</p>
                <p>如果你手里是"34568"，你会觉得"这把悬了"，估值就很低（v=10）。</p>
                <p>注意：这个估值取决于你的牌技（策略 $\pi$）。如果是高手拿烂牌，可能估值也不低；小白拿好牌也可能打输。</p>
            </div>

            <div class="box-flow">
                <p><b>2.1 状态价值</b> 只看"在哪" -> 但做决策还要看"做什么" -> 这就是 <b>2.2 动作价值 Q</b>。</p>
            </div>

            <h3>2.2 动作价值函数 $q_\pi$</h3>
            <div class="box-def">
                <strong>定义 2.2 (动作价值)</strong>
                $$ q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t=s, A_t=a] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$q_\pi(s, a)$: 在状态 $s$ 做了动作 $a$ 之后，且后续继续按照 $\pi$ 玩，平均能拿多少分。</li>
                        <li>它比 $v(s)$ 多了个 $a$，所以它能够直接指导我们该选哪个动作（选那个 Q 最大的）。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>$V(s)$ 是评价"处境"，$Q(s, a)$ 是评价"招数"。</p>
                <p>比如在十字路口（状态 $s$）。</p>
                <p>- 动作 $a_1$：向左转 -> 堵车（分低）。</p>
                <p>- 动作 $a_2$：向右转 -> 畅通（分高）。</p>
                <p>Q函数就是告诉你：在这儿，向左转值50分，向右转值90分。那你肯定选右转。</p>
            </div>

            <div class="box-flow">
                <p><b>2.2 动作价值 Q</b> 定义好了 -> 但怎么算它们？用递推公式 -> 这就是 <b>2.3 贝尔曼方程</b>。</p>
            </div>

            <h3>2.3 贝尔曼期望方程</h3>
            <div class="box-thm">
                <strong>定理 2.1 (递推公式)</strong>
                $$ v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$v_\pi(s)$: 今天的价值。</li>
                        <li>$r$: 今天当下拿到的钱。</li>
                        <li>$\gamma v_\pi(s')$: 明天的价值（打了个折）。</li>
                        <li>$\sum$: 把所有可能发生的明天，按概率加权求和。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>这个公式深刻地揭示了<b>价值的来源</b>：</p>
                <p><b>今天的身价 = 手里的现金 + 未来身价的折现。</b></p>
                <p>这就像你评估一家公司的市值：不仅看它今年甚至今天赚了多少钱（$r$），更要看它未来的潜力（$\gamma
                    v(s')$）。这就把一个无限长的问题，转化成了"今天"和"明天"的关系，让我们能够用迭代法求解。</p>
            </div>
        </section>

        <!-- CHAPTER 3 -->
        <section id="ch3">
            <div class="source-tag">来源：第3课 Bellman Optimality Equation</div>
            <h2>第3章：贝尔曼最优方程 (BOE)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> 第2章评估的是"给定策略的价值"。但我们真正想要的是"最好策略是多少分" -> 区别在于把 $\sum \pi$ 换成 $\max$。</p>
            </div>

            <h3>3.1 最优价值函数</h3>
            <div class="box-def">
                <strong>定义 3.1</strong>
                $$ v_*(s) = \max_\pi v_\pi(s), \quad q_*(s,a) = \max_\pi q_\pi(s,a) $$
            </div>

            <div class="box-flow">
                <p><b>3.1 最优价值定义</b> 告诉我们"理论天花板" -> 怎么通过递推公式计算？ -> 这就是 <b>3.2 BOE</b>。</p>
            </div>

            <h3>3.2 贝尔曼最优方程</h3>
            <div class="box-thm">
                <strong>定理 3.1 (BOE)</strong>
                $$ v_*(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$v_*(s)$: 理论上的最高身价（神之操作下的得分）。</li>
                        <li>$\max_a$: <b>最大化</b>。这里不再是"按概率求和"（期望），而是"我只选那个能让我未来分最高的动作"。</li>
                        <li>$v_*(s')$: 假设明天我也是用神之操作（Bootstrapping）。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>之前的 Bellman Equation 是描述"凡人"的：我很平庸，有时走左有时走右，所以我算个平均分。</p>
                <p>Bellman Optimality Equation (BOE) 是描述"神"的：<b>我每一步都做出了最正确的选择。</b></p>
                <p>公式里的 $\max$ 就是这种"神性"的体现：在当前路口，我绝对不会瞎选，我一定选那条通往金矿的路。只要我每一步都贪心地选最好的，我的总分就是全宇宙最高的。</p>
            </div>
        </section>

        <!-- CHAPTER 4 -->
        <section id="ch4">
            <div class="source-tag">来源：第4课 Dynamic Programming</div>
            <h2>第4章：动态规划 (DP)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> 第3章给出了方程 (BOE)，但它是非线性的，很难解。DP 说：如果有上帝视角（知道 $P$、$R$），就可以用迭代法逼近解。</p>
            </div>

            <h3>4.1 策略评估 (Policy Evaluation)</h3>
            <div class="box-def">
                $$ v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma v_k(s')] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$v_k$: 第 k 轮的估值表。</li>
                        <li>$v_{k+1}$: 第 k+1 轮的估值表。</li>
                        <li>$\leftarrow$: 通过不断把 $v_k$ 代入右边算出 $v_{k+1}$，数值会慢慢稳定下来，最终变成真实的 $v_\pi$。</li>
                    </ul>
                </div>
            </div>

            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>这就像是<b>"互相打听身价"</b>。</p>
                <p>一开始大家都是瞎猜的（比如都猜0分）。</p>
                <p>第一轮：你看看你手里拿了多少工资 $r$，再问问你邻居（下一站）瞎猜的身价 $v_0$。</p>
                <p>第二轮：你邻居通过第一轮更新，好像准了一点。你再根据他的新身价 $v_1$ 更新自己。</p>
                <p><b>为什么这叫 Model-based？</b> 因为你必须知道"谁是你邻居"以及"去邻居家的概率"（即知道 $P$）。如果你连地图都没有（不知道
                    $P$），你就没法问邻居。现实中很多时候我们没有地图，所以 DP 往往只存在于理论中。</p>
            </div>

            <div class="box-flow">
                <p><b>4.1 策略评估</b> 告诉我们策略有多好 -> 如何变得更好？ -> 这就是 <b>4.2 策略改进</b>。</p>
            </div>

            <h3>4.2 策略改进 (Policy Improvement)</h3>
            <div class="box-thm">
                $$ \pi'(s) = \arg\max_a q_\pi(s, a) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$q_\pi(s, a)$: 既然我已经评估出了当前的 $v$，我就能算出每个动作的 $q$。</li>
                        <li>$\arg\max_a$: 我不再随机了，我把那个分最高的动作定为我的新策略 $\pi'$。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>这就是<b>"择优而从"</b>。</p>
                <p>评估完了发现：走左边预期50分，走右边预期80分。</p>
                <p>那我傻啊我还走左边？我立马改策略：<b>以后到这就是无脑走右边！</b></p>
                <p>这就是策略改进 — 只要我贪心地选好的，新策略一定比旧策略强。</p>
            </div>


            <div class="box-flow">
                <p><b>4.1 策略评估</b> + <b>4.2 策略改进</b> 循环交替 = <b>4.3 策略迭代</b>。</p>
            </div>

            <h3>4.3 策略迭代 (Policy Iteration)</h3>
            <div class="box-def">
                <strong>算法流程</strong>
                <ul>
                    <li><b>第一步（评估）：</b> 保持策略 $\pi$ 不变，反复迭代计算 $v_\pi$ 直到收敛（也可以直接解方程）。</li>
                    <li><b>第二步（改进）：</b> 根据准确的 $v_\pi$ 算出 $q_\pi$，更新策略 $\pi'(s) = \arg\max_a q_\pi(s,a)$。</li>
                    <li>重复上述两步，直到策略不再变化（收敛至最优）。</li>
                </ul>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p><b>策略迭代</b> 是个"完美主义者"。</p>
                <p>它每一次都要先把当下的能力彻底摸透（评估到收敛），确认无误了，才敢迈出改进的一步。</p>
                <p>虽然走得稳，但太墨迹了。能不能别等算准了再改？</p>
            </div>

            <div class="box-flow">
                <p><b>4.3 策略迭代</b> 太慢（因为要评估到收敛） -> 如果只评估一步就立刻改进？ -> 这就是 <b>4.4 值迭代</b>。</p>
            </div>

            <h3>4.4 值迭代 (Value Iteration)</h3>
            <div class="box-def">
                <strong>迭代公式</strong>
                $$ v_{k+1}(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$\max_a$: 注意这里直接取了 max。</li>
                        <li>本质：相当于把"评估"和"改进"合二为一。每一轮更新，$v$ 值变了，隐含的贪心策略其实也跟着变了。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p><b>值迭代</b> 是个"急脾气"。</p>
                <p>它不求把当下的身价算准，它只想赶紧变强。</p>
                <p>看一眼局势，觉得哪边好就往哪边改一点。虽然中间过程的身价表 ($v$) 可能不准，但只要一直在往好的方向挪，最后也能殊途同归找到最优解。</p>
            </div>
        </section>

        <!-- CHAPTER 5 -->
        <section id="ch5">
            <div class="source-tag">来源：第5课 Monte Carlo Methods</div>
            <h2>第5章：蒙特卡洛方法 (MC)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> DP 需要知道 $P$ 和 $R$（上帝视角）。现实中我们往往不知道！MC 说：不知道就<b>试</b>，试很多次取平均。</p>
            </div>

            <h3>5.1 MC 策略评估</h3>
            <div class="box-def">
                $$ v_\pi(s) \approx \frac{1}{N} \sum_{i=1}^N G_i $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$N$: 我从这个状态出发，玩了多少局游戏。</li>
                        <li>$G_i$: 第 i 局游戏实际拿到的总分。</li>
                        <li>$\frac{1}{N} \sum$: 算术平均。</li>
                    </ul>
                </div>
            </div>

            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>DP 需要地图 ($P$)，MC 不需要。MC 是<b>"实践出真知" (Model-free)</b>。</p>
                <p><b>为什么是 Model-free？</b> 我不需要知道物理引擎 ($P$) 怎么编写，我只需要跳进去玩。玩一局，我就知道这一局得了多少分。只要我玩的次数够多，平均分就是真实价值。</p>
                <p><b>为什么必须等到游戏结束？</b> 因为定义 $G_t$ 是"未来所有奖励的总和"。如果不打到这一局结束，你就不知道总分是多少，也就没法拿这个数据去更新你的估值。所以 MC 只能用于<b>有终点
                        (Episodic)</b> 的任务。</p>
            </div>


            <div class="box-flow">
                <p><b>5.1 策略评估</b> (利用) 告诉我们哪个好 -> 但若只选目前最好的可能错过更好的 -> 需要 <b>5.2 探索</b> ($\epsilon$-greedy)。</p>
            </div>

            <h3>5.2 探索与利用 (Exploration vs Exploitation)</h3>
            <div class="box-def">
                <p><b>ε-greedy 策略</b>：以 $1-\epsilon$ 概率选最优动作，以 $\epsilon$ 概率随机选。</p>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>这是一个<b>"吃货的困境"</b>：</p>
                <p>你家楼下有家馆子很好吃（Exploitation - 利用）。</p>
                <p>但是如果你天天只吃这一家，你就永远不知道隔壁新开的那家是不是更好吃。</p>
                <p>所以你必须偶尔（$\epsilon$ 的概率）去尝试一下新馆子，或者瞎吃一家（Exploration - 探索）。哪怕可能踩雷，但为了发现潜在的米其林三星，这是值得的。</p>
            </div>
        </section>

        <!-- CHAPTER 6 -->
        <section id="ch6">
            <div class="source-tag">来源：第6课 Stochastic Approximation</div>
            <h2>第6章：随机逼近与 SGD</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> MC 需要存储所有 $G_i$ 再取平均，内存占用大。有没有办法<b>边走边算</b>？随机逼近 (SA) / SGD 就是答案。这是 TD 的数学基础。</p>
            </div>

            <h3>6.1 Robbins-Monro 算法</h3>
            <div class="box-def">
                <strong>迭代公式</strong>
                $$ w_{k+1} = w_k - \alpha_k \tilde{g}(w_k, \eta_k) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$w_k$: 当前的猜测值。</li>
                        <li>$\alpha_k$: 步长。如果步长太大，会震荡；如果太小，走不动。</li>
                        <li>$\tilde{g}$: 加上了噪声的梯度方向（因为我们只有样本，没有全貌）。</li>
                    </ul>
                </div>
            </div>

            <div class="box-easy">
                <strong>通俗解读 (它解决了什么核心难题？)</strong>
                <p>在 RL 中，我们想求期望（平均值），但我们不知道概率分布 $P$。<b>Robbins-Monro 算法告诉我们：不需要算期望！</b></p>
                <p>只要你给我一个<b>采样</b>（比如玩一次游戏的一条轨迹），我就能拿着这个样本去更新参数。虽然单个样本有噪声（不准），但只要按照 RM 算法的规则（步长 $\alpha$
                    越来越小），最后就能神奇地收敛到真实的期望值。</p>
                <p>这就打通了从"理论计算"（需要 $P$）到"数据学习"（只需要样本）的任督二脉。</p>
            </div>


            <div class="box-flow">
                <p><b>6.1 RM算法</b> 是理论基础 -> 把它搬到神经网络优化问题上 -> 就变成了 <b>6.2 SGD</b>。</p>
            </div>

            <h3>6.2 随机梯度下降 (SGD)</h3>
            <div class="box-def">
                $$ \theta_{t+1} = \theta_t - \alpha \nabla_\theta L(\theta_t; x_t, y_t) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$\nabla L$: 损失函数的梯度。</li>
                        <li>$(x_t, y_t)$: 仅仅使用一个样本计算梯度，而不是用整个数据集 (Batch)。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>SGD 就是深度学习界的 Robbins-Monro 算法。</p>
                <p>要是每次算梯度都要把几百万张图片过一遍，电脑早死机了。SGD 说：虽然单张图片的梯度很不准（有噪声），但我看一张图就改一次参数，改它个几亿次，最终效果跟全看一遍是一样的，而且快得多！</p>
            </div>
        </section>

        <!-- CHAPTER 7 -->
        <section id="ch7">
            <div class="source-tag">来源：第7课 Temporal Difference Learning</div>
            <h2>第7章：时序差分 (TD)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> MC 必须等到游戏结束才能更新（慢）。TD 结合了 DP 和 MC 的优点：</p>
                <ul>
                    <li><b>像 MC (Model-free)</b>：直接从经验中学习，不需要知道环境概率 $P$。</li>
                    <li><b>像 DP (Bootstrapping)</b>：用后继状态的估计值 $V(S')$ 来更新当前 $V(S)$，不用等到底。</li>
                </ul>
                <p>结果：TD 可以<b>走一步更新一步</b>，速度快且方差比 MC 小。</p>
                <br>
                <p><b>💡 TD 家族谱系：</b></p>
                <ul>
                    <li><b>TD(0)</b>: 解决<b>预测问题</b> (Prediction)。只评估 V，不负责找动作。</li>
                    <li><b>SARSA</b>: 解决<b>控制问题</b> (Control)。边走边学，保守派 (On-Policy)。</li>
                    <li><b>Q-Learning</b>: 解决<b>控制问题</b> (Control)。边走边学，激进派 (Off-Policy)。</li>
                </ul>
            </div>

            <h3>7.1 TD(0) 公式</h3>
            <div class="box-def">
                $$ V(S_t) \leftarrow V(S_t) + \alpha [ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) ] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$V(S_t)$: 旧的预测。</li>
                        <li>$R_{t+1} + \gamma V(S_{t+1})$: <b>TD Target</b>。这是一种"更靠谱一点"的新猜测。因为它包含了真实发生了一步的奖励 $R$。</li>
                        <li>$[\dots]$: 括号里的是 <b>TD Error</b>（偏差）。即"现实打脸了预测"。</li>
                        <li>$\alpha$: 认错态度。如果 $\alpha$ 很大，说明我听到新消息立马大幅修改旧观念。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>MC 是<b>"秋后算账"</b>：非要等到人死了盖棺定论，才回去评价他这一生。</p>
                <p>TD 是<b>"随机应变"</b>：</p>
                <p>你预测开车回家要30分钟。</p>
                <p>开了5分钟到路口，发现前面修路堵死。你立马修正预测：现在看来至少还要40分钟（总共45分钟）。</p>
                <p>你不需要真的开回家，你只需要开到"下一个路口"，有了新的信息，就能修正之前的预测。这就是 TD：<b>Bootstrapping（用估计修正估计）</b>。</p>
            </div>


            <div class="box-flow">
                <p><b>7.1 TD(0)</b> 能估算 V -> 但做决策需要知道 Q -> 于是此时用 TD 估算 Q，就得到了 <b>7.2 SARSA</b>。</p>
            </div>

            <h3>7.2 SARSA (On-Policy)</h3>
            <div class="box-def">
                $$ Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma Q(S', A') - Q(S,A)] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$S, A, R, S', A'$: 这个公式用到了五元组，所以叫 SARSA。</li>
                        <li>$Q(S', A')$: 这里的 $A'$ 是我<b>实际</b>走出这一步后，下一步<b>真的</b>打算做的动作。很老实。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>SARSA 是个<b>"老实人"</b>。</p>
                <p>它在学的时候，会由于胆小（$\epsilon$-greedy）而偶尔走错路（探索）。但在更新 Q 值的时候，它会把这个"走错路"的后果老老实实记下来。</p>
                <p>因次，SARSA 比较谨慎，会尽量避开悬崖边走，因为它怕自己手抖掉下去。</p>
            </div>


            <div class="box-flow">
                <p><b>7.2 SARSA</b> 太谨慎 (On-Policy) -> 如果想要大胆探索最优路径？ -> 让更新目标不仅限于当前动作 -> <b>7.3 Q-Learning</b>。</p>
            </div>

            <h3>7.3 Q-Learning (Off-Policy)</h3>
            <div class="box-def">
                $$ Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma \max_{a} Q(S', a) - Q(S,A)] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$\max_{a} Q(S', a)$: 这里的区别在于，不管我下一步实际做了啥（哪怕我乱走了），在更新计算时，我<b>假设</b>我下一步会做最完美的动作。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>Q-Learning 是个<b>"梦想家"</b>。</p>
                <p>虽然我现在在瞎走（探索），但我心怀理想。我认为我每一步之后，都会走上最优路径 ($\max Q$)。</p>
                <p>所以 Q-Learning 不怕走悬崖边。它想："虽然我可能会手抖掉下去，但只要不掉下去，那条路就是最近的！" 所以它会找到更优的路径，但也比 SARSA 更鲁莽。</p>
            </div>

            <div class="box-flow">
                <p><b>7.3 Q-Learning</b> 只看眼前这一步。如果我想看远一点，比如看3步再更新？ -> <b>n-Step TD</b>。如果我想综合看1步、2步...直到无穷步？ ->
                    <b>TD($\lambda$)</b>。
                </p>
            </div>

            <h3>7.4 n-Step TD 与 TD($\lambda$)</h3>
            <div class="box-def">
                <strong>n-Step TD</strong>
                <p>不再可以只看 $R_{t+1}$，而是多看几步真实奖励，最后再加上估计值。</p>
                <ul>
                    <li>TD(0): $R_{t+1} + \gamma V_t(S_{t+1})$ (只看1步真实)</li>
                    <li>MC: $R_{t+1} + \dots + \gamma^{T-1}R_T$ (全看真实)</li>
                    <li>n-Step: $R_{t+1} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n V_t(S_{t+n})$ (看n步真实)</li>
                </ul>
                <br>
                <strong>TD($\lambda$)</strong>
                <p>$\lambda$ 是一个权重参数 ($\lambda \in [0, 1]$)。TD($\lambda$) 相当于把所有可能的 n-Step Return 加权平均。</p>
                <ul>
                    <li>$\lambda = 0$: 变成了 TD(0)。只看眼前，<b>Bias 大 (偏差大)</b>，<b>Variance 小 (方差小)</b>。</li>
                    <li>$\lambda = 1$: 变成了 MC。全看未来，<b>Bias 小 (无偏)</b>，<b>Variance 大 (方差大)</b>。</li>
                    <li>$0 < \lambda < 1$: 取得了中间平衡，既利用了真实回报的无偏性，又利用了估值的稳定性。</li>
                </ul>
            </div>
        </section>

        <!-- CHAPTER 8 -->
        <section id="ch8">
            <div class="source-tag">来源：第8课 Value Function Approximation</div>
            <h2>第8章：值函数近似 (DQN)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> 第4章到第7章我们讲的方法，统称为<b>表格法 (Tabular Methods)</b>。</p>
                <p><b>什么是表格法？</b> 就是用一个超级大的 Excel 表来存 $Q(s,a)$。一行是状态 s，一列是动作 a。</p>
                <p><b>有问题吗？</b> 有。如果状态 s 是围棋的棋盘（$10^{170}$ 种可能），Excel 表直接爆炸了，存不下也查不完（维度灾难）。</p>
                <p><b>怎么解决？</b> 用一个函数（神经网络）$f(s)$ 来把这个表"压缩"进去。这就是<b>值函数近似</b>。</p>
            </div>

            <h3>8.1 函数近似</h3>
            <div class="box-def">
                $$ \hat{v}(s, w) \approx v_\pi(s) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$\hat{v}$: 估计函数（神经网络）。</li>
                        <li>$w$: 神经网络的参数（权重）。</li>
                        <li>目标：找到一组参数 $w$，让 $\hat{v}$ 和真实的 $v$ 越像越好。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>这就是<b>"举一反三"</b>。</p>
                <p>之前的表格法是死记硬背：背下9x9乘法表。如果问你 19x19，你就傻了。</p>
                <p>函数近似是学规律：学会运算口诀（参数 $w$）。哪怕遇到没见过的数字（新状态），套进公式（神经网络）也能算个八九不离十。这就是深度强化学习 (Deep RL) 的由来。</p>
            </div>


            <div class="box-flow">
                <p><b>8.1 函数近似</b> 引入了神经网络，但不稳定 -> 引入两大神技来"稳住"它 -> <b>8.2 经验回放 & 目标网络</b>。</p>
            </div>

            <h3>8.2 DQN 的两大法宝</h3>
            <div class="box-def">
                <strong>(1) Experience Replay (经验回放)</strong>
                <p><b>学术原理：</b> 使得数据满足 <b>I.I.D (独立同分布)</b> 假设。</p>
                <ul>
                    <li><b>问题：</b> RL 的数据是序列产生 ($s_t, s_{t+1}, \dots$)，前后样本高度相关。且随着策略改变，数据分布也在变
                        (Non-stationary)。这违反了神经网络训练最基本的 I.I.D 假设，导致训练发散。</li>
                    <li><b>解决：</b> 将 $(s,a,r,s')$ 存入 Replay Buffer，训练时<b>随机采样 (Random Batch)</b>。这打破了时间相关性，使梯度下降更稳定。
                    </li>
                </ul>
                <br>
                <strong>(2) Target Network (目标网络)</strong>
                <p><b>学术原理：</b> 解决 <b>Moving Target (移动目标)</b> 问题，抑制振荡。</p>
                <ul>
                    <li><b>问题：</b> 我们的损失函数是 $L(\theta) = (r + \gamma \max Q(s'; \theta) - Q(s; \theta))^2$。注意，target
                        本身也包含参数 $\theta$。每次更新 $\theta$，这就好比"左脚踩右脚"，目标和预测都在变，容易追尾（震荡）。</li>
                    <li><b>解决：</b> 引入另一套参数 $\theta^-$ 固定一段时间。Target 变成 $r + \gamma \max Q(s';
                        \theta^-)$。这就把"打移动靶"变成了"打固定靶"，每隔几千步再把靶子挪一下 ($\theta^- \leftarrow \theta$)。</li>
                </ul>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>把深度学习用到 RL 上并不容易，因为数据是关联的且不稳定的。DQN 发明了两招：</p>
                <p>1. <b>经验回放</b>：就是"错题本"。不要做完一道题就扔，记在本子上。晚上复习的时候，打乱顺序抽着做。这样既利用了旧数据，又打破了数据的连续性（关联性），模拟了"独立"做题的环境。</p>
                <p>2. <b>目标网络</b>：就是"打移动靶"。如果不固定 Target，你一更新参数，目标也跟着变，这永远练不准。Target Network
                    也就是把靶子钉死一会，等你练好了，再把靶子挪个位置继续练。</p>
            </div>
        </section>

        <!-- CHAPTER 9 -->
        <section id="ch9">
            <div class="source-tag">来源：第9课 Policy Gradient</div>
            <h2>第9章：策略梯度 (Policy Gradient)</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> DQN 虽然强，但只能处理<b>离散动作</b>（如按键：上、下、左、右）。</p>
                <p><b>为什么 DQN 搞不定连续动作？</b> 因为 DQN 的决策依据是 $a^* = \arg\max_a Q(s,a)$。</p>
                <ul>
                    <li>离散时：比大小就行（比如比4个动作的Q值）。</li>
                    <li>连续时（如方向盘转角 0°~360°）：要在一个连续无限空间里找最大值，这本身就是个极难的优化问题，不可能每一步都算一遍。</li>
                </ul>
                <p><b>怎么办？</b> 不要算 Q 再找 Max 了，直接输出动作！这就是 <b>策略梯度 (PG)</b>。</p>
            </div>

            <h3>9.1 策略梯度定理</h3>
            <div class="box-thm">
                $$ \nabla_\theta J(\theta) \propto \mathbb{E}_\pi [ Q^\pi(s,a) \nabla_\theta \ln \pi(a|s, \theta) ] $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>$J(\theta)$: 策略的好坏（目标函数）。</li>
                        <li>$\nabla \ln \pi$: 告诉你往哪个方向调参数，能让动作 $a$ 出现的概率变大。</li>
                        <li>$Q^\pi$: 这个动作 $a$ 到底好不好。</li>
                        <li>意义：如果 $Q$ 很大（动作好），我们就沿着梯度方向走（增加概率）；如果 $Q$ 很小，反之。</li>
                    </ul>
                </div>
            </div>

            <div class="box-flow">
                <p><b>9.1 策略梯度定理</b> 是理论 -> 实际怎么估计 Q？用 MC 回报 -> 这就是 <b>9.2 REINFORCE</b>。</p>
            </div>

            <h3>9.2 REINFORCE</h3>
            <div class="box-def">
                $$ \theta \leftarrow \theta + \alpha G_t \nabla \ln \pi(A_t|S_t, \theta) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 公式详解：</span>
                    <ul>
                        <li>这里直接用 $G_t$（一整局的实际得分）代替了 $Q$。</li>
                        <li>这是一种蒙特卡洛方法，必须等游戏结束才能更新。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>这就是<b>"不论过程，只看结果"</b>。</p>
                <p>虽然我不知道哪一步做对了，但只要最后赢了（$G_t$ 高），我就认为这一局里我做的所有动作都是对的，全部增加概率！</p>
                <p>虽然会有误杀（赢的那局里可能也有臭棋），但只要样本够多，平均下来只有真正的妙手会被保留。</p>
            </div>
        </section>

        <!-- CHAPTER 10 -->
        <section id="ch10">
            <div class="source-tag">来源：第10课 Actor-Critic</div>
            <h2>第10章：Actor-Critic</h2>

            <div class="box-logic">
                <p><b>🔗 承上启下：</b> REINFORCE 用整局回报 $G_t$，方差太大。如果用 TD 思想实时打分呢？这就诞生了 Actor-Critic。</p>
            </div>

            <h3>10.1 架构</h3>
            <div class="mermaid">
                graph LR
                Actor["Actor (演员)<br>输出动作"] -- "action" --> Env["Environment"]
                Env -- "reward, state'" --> Critic["Critic (评委)<br>实时打分"]
                Critic -- "TD Error" --> Actor
                style Actor fill:#e1f5fe,stroke:#01579b
                style Critic fill:#fff3e0,stroke:#e65100
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p><b>Actor</b> 是运动员，负责比赛。</p>
                <p><b>Critic</b> 是教练，负责打分。</p>
                <p>如果是 REINFORCE，没教练，运动员只能瞎练，等比赛赢了才知道自己练没练对。</p>
                <p>有了 Critic，运动员每做一个动作，教练立马喊："这姿势不对！扣分！"（TD Error）。运动员原地就能改，学习效率极大提高。</p>
            </div>


            <div class="box-flow">
                <p><b>10.1 架构</b> 引入了 Critic -> 但 Critic 打绝对分数方差大 -> 改成打"相对分数" (Advantage) -> <b>10.2 A2C</b>。</p>
            </div>

            <h3>10.2 Advantage Actor-Critic (A2C)</h3>
            <div class="box-logic">
                <p><b>💡 为什么要区分两个更新目标？</b></p>
                <ul>
                    <li><b>Actor (策略)</b> 需要 <b>Advantage</b>：它需要知道"这个动作比平庸水平好多少"，这样才能决定是增加概率还是减少概率。A 是<b>相对值</b>。</li>
                    <li><b>Critic (评委)</b> 需要 <b>Target</b>：它需要知道"这一局真实的绝对得分是多少"，这样它才能训练自己打分越来越准。Target 是<b>绝对值</b>。
                    </li>
                </ul>
            </div>
            <div class="box-def">
                <strong>(1) 优势函数 (Advantage Function) 定义</strong>
                $$ A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 数学本质：</span>
                    <ul>
                        <li>$Q^\pi(s,a)$: 在状态 $s$ 做动作 $a$ 的价值。</li>
                        <li>$V^\pi(s)$: 在状态 $s$ 平均能拿多少分（即 $\sum \pi(a|s) Q(s,a)$）。</li>
                        <li><b>物理意义：</b> 动作 $a$ 相对于期望表现好多少？如果 $A>0$，说明 $a$ 是好动作（高于平均）；$A<0$，说明是坏动作。< /li>
                    </ul>
                </div>
                <br>
                <strong>(2) 实际计算：用 TD Error 近似 Advantage</strong>
                <p>实际训练时，我们不想维护兩個网络 (Q 和 V) 因为太累赘。我们用 TD Error $\delta$ 来估算 $A$：</p>
                $$ A(s_t, a_t) \approx \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$
                <div class="formula-breakdown">
                    <span class="formula-breakdown-title">📝 为什么可以这样代替？</span>
                    <ul>
                        <li>因为 $Q(s_t, a_t) \approx r_t + \gamma V(s_{t+1})$ (贝尔曼方程)。</li>
                        <li>所以 $Q - V \approx (r + \gamma V') - V$。这就是 TD Error！</li>
                        <li><b>妙处：</b> 我们只需要训练一个 Critic 网络 $V(s)$，不仅能算 V，还能顺便算出 A。</li>
                    </ul>
                </div>
            </div>
            <div class="box-easy">
                <strong>通俗解读</strong>
                <p>为什么要减去 $V(s)$？这叫<b>"减去基线 (Baseline) 降方差"</b>。</p>
                <p><b>举例：</b> 假设不管做什么动作，最少都给100分（$Q \in [100, 105]$）。</p>
                <ul>
                    <li>如果不减基线：$Q$ 值很大，梯度一直很大，更新很不稳（方差大）。</li>
                    <li>减去基线 $V \approx 102$：优势 $A \in [-2, +3]$。</li>
                </ul>
                <p>如果 $A$ 是正的（+3），就鼓励；如果是负的（-2），就惩罚。<b>奖罚分明</b>，模型学得更快更稳。</p>
            </div>

            <div class="box-logic" style="background-color: #fff3e0; border-left-color: #ff9800;">
                <p><b>🚀 进阶：如何理解高深论文中的概念？</b></p>
                <p>例如这句话："引入了 <b>Decoupled GAE</b> 和 <b>Value Pretraining</b> 来解决长序列中的<b>价值估计偏差</b>问题"。</p>
                <p>要读懂它，你需要调用以下章节的知识：</p>
                <ul>
                    <li><b>GAE (Generalized Advantage Estimation)</b>: 即"广义优势估计"。你需要懂 <b>第10章 Advantage</b> 的定义，以及
                        <b>第7章 TD($\lambda$)</b> 中的多步回报权衡（Bias-Variance Tradeoff）。GAE 就是一种更高级的计算 A 值的方法。
                    </li>
                    <li><b>价值估计偏差 (Bias)</b>: 你需要懂 <b>第5章 MC (无偏)</b> 和 <b>第7章 TD (有偏)</b> 的区别。长序列中，TD 的偏差会累积，所以需要技巧来修正。
                    </li>
                    <li><b>Value Pretraining</b>: 你需要懂 <b>第8章 函数近似</b>。即在正式学策略前，先用神经网络把 V 值（裁判）练得准一点，别让瞎指挥的裁判带偏了运动员。
                    </li>
                </ul>
                <hr style="border-top: 1px dashed #ff9800; margin: 10px 0;">
                <p>例如这句话："在标准 PPO 中，计算 <b>Policy 的优势</b> 和计算 <b>Value 的目标</b> 通常共用同一个 $\lambda$"。</p>
                <p>要读懂它，你需要调用以下章节的知识：</p>
                <ul>
                    <li><b>共用 $\lambda$ (Lambda)</b>: 你需要懂 <b>第7章 TD($\lambda$)</b>。$\lambda$ 是控 Bias 和 Variance 平衡的。共用
                        $\lambda$ 是为了让 Actor 和 Critic 对"未来的看法"保持一致。</li>
                    <li><b>Policy 的优势</b>: 即 Actor 更新用的 $A$ (<b>第10章</b>)。在 PPO 中，通常用 GAE($\lambda$) 来算 A。</li>
                    <li><b>Value 的目标</b>: 即 Critic 更新用的 Target (<b>第7章 & 8章</b>)。 Critic 需要拟合"真实回报"，这里通常直接用计算 A 时用到的
                        $TD(\lambda)$ 回报作为目标。</li>
                </ul>
            </div>
        </section>

        <footer>
            <p>整理自 赵世钰《强化学习的数学原理》系列课程。</p>
            <p>Built with ❤️ by AI Assistant</p>
        </footer>

    </main>

</body>

</html>
